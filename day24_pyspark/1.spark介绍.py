"""
date 2025/12/9
@author chryl

Spark是什么
定义:Apache Spark是用于大规模数据(larse-scala data)处理的统-(unified)分析引擎

简单来说，Spark是一款分布式的计算框架，用于调度成百上千的服务器集群，计算TB、PB乃至EB级别的海量数据

Spark作为全球顶级的分布式计算框架,支持众多的编程语言进行开发。而Python语言,则是Spark重点支持的方向。

Spark对Python语言的支持,重点体现在，Python第三方库: PySpark之上。
PySpark是由Spark官方开发的Python语言第三方库。
Python开发者可以使用pip程序快速的安装PySpark并像其它三方库那样直接使用。

PySpark
    ①作为Python库进行数据处理
    ②提交至Spark集群进行分布式集群计算


"""
